{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from https://www.kaggle.com/Cornell-University/arxiv\n",
    "\n",
    "def line_generator(fname='/home/tomek/projects/paper-finder/arxiv-metadata-oai-snapshot.json'):\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "def make_dataframe(n=1000):\n",
    "    li = []\n",
    "    gen = line_generator()\n",
    "    for idx, line in enumerate(gen):\n",
    "        d = json.loads(line)\n",
    "        li.append(d)        \n",
    "        if idx >= n-1: break\n",
    "    \n",
    "    return pd.DataFrame(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller dataframe\n",
    "Make a dataframe that fits into 16GBs of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_dataframe(1e6)\n",
    "df_id_abst = df[['id', 'abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're probably interested in cs.[CV|CL|LG|AI|NE]/stat.ML \n",
    "mask_ai = df.categories.apply(lambda s: 'cs.AI' in s).values\n",
    "mask_cv = df.categories.apply(lambda s: 'cs.CV' in s).values\n",
    "mask_cl = df.categories.apply(lambda s: 'cs.CL' in s).values\n",
    "mask_lg = df.categories.apply(lambda s: 'cs.LG' in s).values\n",
    "mask_ne = df.categories.apply(lambda s: 'cs.NE' in s).values\n",
    "mask_stat_ml = df.categories.apply(lambda s: 'stat.ML ' in s).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_all = np.vstack([mask_ai, mask_cv, mask_cl, mask_lg, mask_ne, mask_stat_ml])\n",
    "mask_all = np.sum(mask_all, axis=0, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roi = df[mask_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_arxiv_id_abst_ai_cv_cl_lg_ne_ml.pkl', 'wb') as f:\n",
    "    pickle.dump(df_roi, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(input='content', \n",
    "        encoding='utf-8', decode_error='replace', strip_accents='unicode', \n",
    "        lowercase=True, analyzer='word', stop_words='english', \n",
    "        token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n",
    "        ngram_range=(1, 2), max_features = None, \n",
    "        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "        max_df=1.0, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 874 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "%time tfidf_v.fit(df.abstract)\n",
    "\n",
    "# Alternatively we can fit the vectorizer only on the subset topic\n",
    "# tfidf_v = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "# %time tfidf_v.fit(df.abstract[mask_cv == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_arxiv_id_abst_full.pkl', 'wb') as f:\n",
    "    pickle.dump(df_id_abst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf_idf_vectorizer_ngram1.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_v.transform(df.abstract)\n",
    "\n",
    "with open('X_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(X_tfidf, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
