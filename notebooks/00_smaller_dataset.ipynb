{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_generator(fname='arxiv-metadata-oai-snapshot.json'):\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "def make_dataframe(n=1000):\n",
    "    li = []\n",
    "    gen = line_generator()\n",
    "    for idx, line in enumerate(gen):\n",
    "        d = json.loads(line)\n",
    "        li.append(d)        \n",
    "        if idx >= n-1: break\n",
    "    \n",
    "    return pd.DataFrame(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.96 ms, sys: 728 Âµs, total: 9.69 ms\n",
      "Wall time: 12.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time df = make_dataframe(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smaller dataframe\n",
    "Make a dataframe that fits into 16GBs of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id_abst = df[['id', 'abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id+abstract.pkl', 'wb') as f:\n",
    "    pickle.dump(df_id_abst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're probably interested in cs.[CV|CL|LG|AI|NE]/stat.ML \n",
    "mask_ai = df.categories.apply(lambda s: 'cs.AI' in s)\n",
    "mask_cv = ai = df.categories.apply(lambda s: 'cs.CV' in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai = df[mask_ai]\n",
    "df_cv = df[mask_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full_ai.pkl', 'wb') as f:\n",
    "    pickle.dump(df_ai, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('full_cv.pkl', 'wb') as f:\n",
    "    pickle.dump(df_cv, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try fitting a tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer(input='content', \n",
    "        encoding='utf-8', decode_error='replace', strip_accents='unicode', \n",
    "        lowercase=True, analyzer='word', stop_words='english', \n",
    "        token_pattern=r'(?u)\\b[a-zA-Z_][a-zA-Z0-9_]+\\b',\n",
    "        ngram_range=(1, 2), max_features = None, \n",
    "        norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "        max_df=1.0, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 1.78 s, total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "%time tfidf_v.fit(df.abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.57 s, sys: 58.7 ms, total: 5.63 s\n",
      "Wall time: 5.64 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_v = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "%time tfidf_v.fit(df.abstract[mask_cv == True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
